volumes:
  hf_cache:

services:
  test:
    image: nvidia/cuda:12.9.0-base-ubuntu22.04
    command: nvidia-smi
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  # gpu-service:
  #   image: nvidia/cuda:11.8.0-base-ubuntu22.04
  #   # The key part for CDI GPU access:
  #   device_cgroup_rules:
  #     - "c 195:* rmw"
  #     - "c 236:* rmw"
  #   devices:
  #     - nvidia.com/gpu=all
  #   command: nvidia-smi

  # # Another example with a specific GPU
  # specific-gpu-service:
  #   image: nvidia/cuda:11.8.0-base-ubuntu22.04
  #   device_cgroup_rules:
  #     - "c 195:* rmw"
  #     - "c 236:* rmw"
  #   devices:
  #     - nvidia.com/gpu=0 # Use only the first GPU
  #   command: nvidia-smi

  embeddings:
    deploy:
      replicas: 4
    image: ghcr.io/huggingface/text-embeddings-inference:86-1.6
    volumes:
      - hf_cache:/.hf_cache
    ports:
      - "8011-8014:8080"
    ipc: host
    # Replace runtime with devices for CDI
    devices:
      - nvidia.com/gpu=all
    device_cgroup_rules:
      - "c 195:* rmw"
      - "c 236:* rmw"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - USE_FLASH_ATTENTION=True
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_HOME=/.hf_cache
      - RUST_LOG=info
      # Performance tuning for embedding models
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - TOKENIZERS_PARALLELISM=true
    restart: no
    env_file: .env
    command:
      [
        "--model-id",
        "jinaai/jina-embeddings-v2-base-de",
        "--hostname",
        "0.0.0.0",
        "--port",
        "8080",
        "--huggingface-hub-cache",
        "/.hf_cache",
        "--tokenization-workers",
        "16",
        "--max-concurrent-requests",
        "1024",
        "--max-batch-tokens",
        "65536",
        "--max-batch-requests",
        "256",
        "--max-client-batch-size",
        "128",
        "--auto-truncate",
        "--payload-limit",
        "10000000",
      ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "1"

  rerankings:
    deploy:
      replicas: 1
    image: ghcr.io/huggingface/text-embeddings-inference:86-1.6
    volumes:
      - hf_cache:/.hf_cache
    ports:
      - 8020:8080
    container_name: rerankings
    env_file: .env
    ipc: host
    # Replace runtime with devices for CDI
    devices:
      - nvidia.com/gpu=all
    device_cgroup_rules:
      - "c 195:* rmw"
      - "c 236:* rmw"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - USE_FLASH_ATTENTION=True
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HF_HOME=/.hf_cache

      - RUST_LOG=info
      # Performance tuning for reranking models
      - OMP_NUM_THREADS=8
      - MKL_NUM_THREADS=8
      - TOKENIZERS_PARALLELISM=true
    restart: no
    command:
      [
        "--model-id",
        "BAAI/bge-reranker-v2-m3",
        "--hostname",
        "0.0.0.0",
        "--port",
        "8080",
        "--huggingface-hub-cache",
        "/.hf_cache",
        "--tokenization-workers",
        "16",
        "--max-concurrent-requests",
        "1024",
        "--max-batch-tokens",
        "32768",
        "--max-batch-requests",
        "256",
        "--max-client-batch-size",
        "64",
        "--auto-truncate",
        "--payload-limit",
        "4000000",
      ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "1"

  vllm:
    deploy:
      replicas: 1
    volumes:
      - hf_cache:/.hf_cache
    restart: no
    image: vllm/vllm-openai:latest
    ports:
      - 7373:80
    device_cgroup_rules:
      - "c 195:* rmw"
      - "c 236:* rmw"
    devices:
      - nvidia.com/gpu=all
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACEHUB_API_TOKEN:-your_hf_api_access_token}
      - HF_HOME=/.hf_cache
      - NVIDIA_VISIBLE_DEVICES=all
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - MAX_PARALLEL_LOADING_WORKERS=4
    ipc: host
    env_file: .env
    command: [
        "--host",
        "0.0.0.0",
        "--port",
        "80",
        "--model",
        "solidrust/Mistral-7B-Instruct-v0.3-AWQ",
        "--served-model-name",
        "uai/lm-small",
        "--max-model-len",
        "4096",
        "--max-num-batched-tokens",
        "4096",
        "--kv-cache-dtype",
        "fp8",
        "--quantization",
        "awq",
        # "--dtype",
        # "float16",
        "--enable-auto-tool-choice",
        "--tool-call-parser",
        "mistral", # llama3_json
        "--chat-template",
        "examples/tool_chat_template_mistral_parallel.jinja", # tool_chat_template_llama3_json
        "--cpu-offload-gb",
        "${LLM_CPU_OFFLOAD_SPACE}",
        "--gpu-memory-utilization",
        "0.55",
        # "--block-size",
        # "32",
        "--swap-space",
        "${LLM_SWAP_SPACE}",
        # "--trust-remote-code",
        "--seed",
        "4269",
        "--max-num-seqs",
        "1",
        "--trust-remote-code",
        "--enable-prefix-caching",
        "--enable-chunked-prefill",
        # "--disable-sliding-window",
        # "--max-paddings",
        # "16",
        # "--enable-chunked-prefill", # Not possible together with prfix caching enabled
        # "--enforce-eager",
        # "--max-parallel-loading-workers",
        # "2"
      ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://vllm:80/health"]
      interval: 30s
      timeout: 5s
      retries: 5
