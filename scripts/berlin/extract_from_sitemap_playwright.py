#!/usr/bin/env python3
"""Bounded Playwright batch runner: extract HTML + text for first N sitemap IDs.

This script loads a Berlin portal discovery snapshot (generated by
`scripts/berlin_portal_discovery.py`), then uses Playwright (Chromium) to render
the first N document URLs and extract:

- best-effort main content HTML
- best-effort main content text

It is designed for *Option A*: "works now" batch extraction via browser render,
without re-implementing the backend API.

Safety / scope:
- bounded number of documents (`--limit`)
- bounded output sizes per doc (`--max-text-chars`, `--max-html-chars`)
- per-doc timeout and optional pacing (`--sleep-seconds`)
- records minimal network metadata (count + endpoint paths optionally) but does
  NOT store cookies/headers/token values
- writes one JSONL file plus optional per-document JSON files under:
  `data/raw/de-state/berlin/playwright-batch/`

Prerequisites:
- Playwright must be installed as a dev dependency:
  `uv add --group dev playwright`
- Chromium must be installed:
  `uv run python -m playwright install chromium`

Linux / Nix note (libgbm / ELF class pitfalls):
- Playwright's downloaded Chromium is 64-bit and requires a 64-bit `libgbm.so.1`.
- In some Nix + FHS environments, you can accidentally pick up a 32-bit `libgbm`
  which fails with:
    `libgbm.so.1: wrong ELF class: ELFCLASS32`
- A reliable workaround is to ensure the FHS `usr/lib64` path is *ahead* of any
  mixed-arch paths. For example (adjust the store path to your current FHS env):
    `LD_LIBRARY_PATH="/nix/store/<...>-legal-mcp-dev-env-fhsenv-rootfs/usr/lib64:$LD_LIBRARY_PATH" ...`

Metadata normalization note:
- `metadata.dokumentkopf` is extracted from the rendered "Dokumentkopf" table.
- For date-like fields, this script performs best-effort normalization to ISO
  `YYYY-MM-DD` while preserving the original string in a parallel `*_raw` key.
  Unknown formats are preserved unchanged.

Example:
    # Batch-extract first 10 sitemap IDs from latest discovery snapshot
    uv run python scripts/berlin/extract_from_sitemap_playwright.py \
      --limit 10 \
      --max-text-chars 200000 \
      --max-html-chars 200000 \
      --sleep-seconds 0.5

    # Use an explicit snapshot file and a different output directory
    uv run python scripts/berlin/extract_from_sitemap_playwright.py \
      --snapshot data/raw/de-state/berlin/discovery/berlin_sitemap_discovery_....json \
      --output-dir data/raw/de-state/berlin/playwright-batch \
      --limit 10

Notes:
- The current discovery snapshot may be dominated by `NJRE...` IDs (likely
  decisions). This script still extracts content, but you should expect the
  DOM/text structure to differ compared to `jlr-...` norms.
- This is intentionally not integrated into ingestion; it produces raw
  extracted artifacts for review.

"""

from __future__ import annotations

import argparse
import contextlib
import json
import re
import sys
import time
from dataclasses import dataclass
from datetime import UTC, date, datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any

from playwright.sync_api import Browser, Page, sync_playwright

if TYPE_CHECKING:
    from collections.abc import Iterable

DEFAULT_DISCOVERY_DIRECTORY = Path("data/raw/de-state/berlin/discovery")
DEFAULT_OUTPUT_DIRECTORY = Path("data/raw/de-state/berlin/playwright-batch")

DEFAULT_LIMIT = 10
DEFAULT_TIMEOUT_SECONDS = 60.0
DEFAULT_WAIT_UNTIL = "networkidle"
DEFAULT_EXTRA_WAIT_MS = 1_000
DEFAULT_SLEEP_SECONDS = 0.5

DEFAULT_MAX_TEXT_CHARS = 200_000
DEFAULT_MAX_HTML_CHARS = 200_000

DEFAULT_USER_AGENT = "legal-mcp-berlin-playwright-batch/0.1 (bounded; research-only)"


class BatchExtractionError(RuntimeError):
    """Raised for failures that should abort the batch run."""


@dataclass(frozen=True, slots=True)
class DiscoveredDocument:
    """A single discovered document entry from a sitemap discovery snapshot."""

    document_id: str
    canonical_url: str


def _utc_now_rfc3339() -> str:
    return datetime.now(UTC).isoformat()


def _clip_text(text: str, max_chars: int) -> str:
    if max_chars <= 0:
        return ""
    if len(text) <= max_chars:
        return text
    return text[:max_chars]


def _dedupe_preserve_order(items: Iterable[str]) -> list[str]:
    seen: set[str] = set()
    output: list[str] = []
    for item in items:
        if item in seen:
            continue
        seen.add(item)
        output.append(item)
    return output


def _ensure_output_directory(output_directory: Path) -> None:
    output_directory.mkdir(parents=True, exist_ok=True)


def _find_latest_snapshot(discovery_directory: Path) -> Path:
    if not discovery_directory.exists():
        raise BatchExtractionError(
            f"Discovery directory not found: {discovery_directory}. "
            "Run scripts/berlin_portal_discovery.py first or pass --snapshot."
        )

    candidates = sorted(
        (path for path in discovery_directory.glob("*.json") if path.is_file()),
        key=lambda path: path.stat().st_mtime,
        reverse=True,
    )
    if not candidates:
        raise BatchExtractionError(
            f"No snapshot JSON files found in {discovery_directory}. "
            "Run scripts/berlin_portal_discovery.py first."
        )
    return candidates[0]


def _load_snapshot(snapshot_path: Path) -> list[DiscoveredDocument]:
    try:
        payload = json.loads(snapshot_path.read_text(encoding="utf-8"))
    except Exception as exception:
        raise BatchExtractionError(
            f"Failed to parse snapshot JSON at {snapshot_path}: {exception}"
        ) from exception

    documents_raw = payload.get("documents")
    if not isinstance(documents_raw, list):
        raise BatchExtractionError(
            f"Snapshot {snapshot_path} missing 'documents' list."
        )

    discovered: list[DiscoveredDocument] = []
    for item in documents_raw:
        if not isinstance(item, dict):
            continue
        document_id = item.get("document_id")
        canonical_url = item.get("canonical_url")
        if not isinstance(document_id, str) or not document_id:
            continue
        if not isinstance(canonical_url, str) or not canonical_url:
            continue
        discovered.append(
            DiscoveredDocument(document_id=document_id, canonical_url=canonical_url)
        )

    if not discovered:
        raise BatchExtractionError(
            f"Snapshot {snapshot_path} contained no valid document entries."
        )

    return discovered


def _best_effort_main_content_selector_candidates() -> list[str]:
    # Heuristic selectors; chosen to work with the bsbe SPA layout.
    return [
        "#main",
        "main",
        "#container",
        "[role='main']",
        "article",
        ".docviewmain",
        ".docframebs__content",
        ".docbody",
    ]


def _extract_minimal_metadata(page: Page, *, canonical_url: str) -> dict[str, Any]:
    """Extract minimal, non-sensitive metadata from the rendered document page.

    This function intentionally avoids collecting any session/cookie/token data.
    It only derives fields from the rendered DOM and the canonical URL.

    Args:
        page: Playwright page after navigation and rendering.
        canonical_url: The canonical document URL (e.g. /bsbe/document/<id>).

    Returns:
        Dict with best-effort metadata fields:
        - title
        - permalink_url
        - pdf_url
        - document_type_prefix ("jlr" | "NJRE" | "other")
        - dokumentkopf (best-effort map parsed from the "Dokumentkopf" table)
    """
    title = ""
    try:
        title_raw = page.title()
        if isinstance(title_raw, str):
            title = title_raw.strip()
    except Exception:
        title = ""

    if not title:
        try:
            header_locator = page.locator(".docLayoutTitel .h3_titel")
            if header_locator.count() > 0:
                title = header_locator.first.inner_text(timeout=1_500).strip()
        except Exception:
            title = ""

    permalink_url = ""
    try:
        perma_locator = page.locator("#juris-document-permalink")
        if perma_locator.count() > 0:
            attribute_value = perma_locator.first.get_attribute("data-href")
            if isinstance(attribute_value, str):
                permalink_url = attribute_value.strip()
    except Exception:
        permalink_url = ""

    if not permalink_url:
        try:
            perma_link_locator = page.locator("a:has-text('perma?d=')")
            if perma_link_locator.count() > 0:
                attribute_value = perma_link_locator.first.get_attribute("href")
                if isinstance(attribute_value, str):
                    permalink_url = attribute_value.strip()
        except Exception:
            permalink_url = ""

    if permalink_url.startswith("/"):
        permalink_url = "https://gesetze.berlin.de" + permalink_url

    pdf_url = ""
    try:
        pdf_locator = page.locator("#juris-document-pdf")
        if pdf_locator.count() > 0:
            attribute_value = pdf_locator.first.get_attribute("data-href")
            if isinstance(attribute_value, str):
                pdf_url = attribute_value.strip()
    except Exception:
        pdf_url = ""

    if pdf_url.startswith("/"):
        pdf_url = "https://gesetze.berlin.de" + pdf_url

    document_type_prefix = "other"
    if "/document/jlr-" in canonical_url:
        document_type_prefix = "jlr"
    elif "/document/NJRE" in canonical_url:
        document_type_prefix = "NJRE"

    dokumentkopf = _extract_dokumentkopf_fields(page)

    return {
        "title": title,
        "permalink_url": permalink_url,
        "pdf_url": pdf_url,
        "document_type_prefix": document_type_prefix,
        "dokumentkopf": dokumentkopf,
    }


def _normalize_dokumentkopf_key(raw_key: str) -> str:
    normalized_key = (
        raw_key.strip()
        .rstrip(":")
        .replace("\u00a0", " ")
        .replace("\t", " ")
        .replace("  ", " ")
    )

    normalized_key_lower = normalized_key.lower()
    if normalized_key_lower.startswith("juris-abk端rzung"):
        return "juris_abbreviation"
    if normalized_key_lower.startswith("amtliche abk端rzung"):
        return "official_abbreviation"
    if normalized_key_lower.startswith("kurzbezeichnung"):
        return "short_title"
    if normalized_key_lower.startswith("ausfertigungsdatum"):
        return "execution_date"
    if normalized_key_lower.startswith("fassung vom"):
        return "version_date"
    if normalized_key_lower.startswith("g端ltig ab"):
        return "valid_from"
    if normalized_key_lower.startswith("g端ltig bis"):
        return "valid_to"
    if normalized_key_lower.startswith("dokumenttyp"):
        return "document_type"
    if normalized_key_lower.startswith("gliederungs-nr"):
        return "classification_number"
    if normalized_key_lower.startswith("quelle"):
        return "source"
    if normalized_key_lower.startswith("fundstelle"):
        return "citation"

    # Decisions (NJRE...) and other portal content often uses a different set of keys.
    if normalized_key_lower == "gericht":
        return "court"
    if normalized_key_lower == "entscheidungsdatum":
        return "decision_date"
    if normalized_key_lower == "rechtskraft":
        return "is_final"
    if normalized_key_lower == "aktenzeichen":
        return "file_number"
    if normalized_key_lower == "ecli":
        return "ecli"
    if normalized_key_lower in {"norm", "normen"}:
        return "referenced_norms"

    return (
        normalized_key_lower.replace("-", "_")
        .replace(" ", "_")
        .replace("__", "_")
        .replace("/", "_")
    )


def _extract_dokumentkopf_fields(page: Page) -> dict[str, Any]:
    """Extract key/value fields from the "Dokumentkopf" header table.

    This is a best-effort extraction based on the rendered DOM structure used by
    the Berlin portal. It should be resilient across norm/decision pages, but
    will return an empty dict if no expected structure is found.

    Args:
        page: Playwright page after navigation and rendering.

    Returns:
        Dict mapping normalized field keys to their values.
        Some values may be normalized to non-string types (e.g. booleans).
    """
    fields: dict[str, Any] = {}

    try:
        header_locator = page.locator("#ID_DOCHEADER .documentHeader")
        if header_locator.count() <= 0:
            return {}
        table_rows = header_locator.locator("tr")
        row_count = table_rows.count()
    except Exception:
        return {}

    for row_index in range(row_count):
        try:
            row = table_rows.nth(row_index)
            header_cell = row.locator("th").first
            value_cell = row.locator("td").first

            raw_key = header_cell.inner_text(timeout=1_000)
            raw_value = value_cell.inner_text(timeout=1_000)

            if not isinstance(raw_key, str) or not isinstance(raw_value, str):
                continue

            key = _normalize_dokumentkopf_key(raw_key)
            value = raw_value.strip().replace("\u00a0", " ")
            if not key or not value:
                continue

            fields[key] = value
        except Exception:
            continue

    fields = _normalize_dokumentkopf_values(fields)

    return fields


def _normalize_dokumentkopf_values(fields: dict[str, Any]) -> dict[str, Any]:
    """Normalize selected Dokumentkopf values to more useful types.

    Normalizations are best-effort and intentionally conservative:
    - If a value matches a known pattern, we normalize it.
    - Otherwise, we preserve the original as-is (no guessing).

    Always-applied, low-risk normalization:
    - Trim strings and normalize whitespace (incl. NBSP, tabs, newlines) to single spaces.

    Current type normalizations:
    - `is_final` (from NJRE decisions) to a boolean when it contains ja/nein-style values.
    - Date-like fields to ISO `YYYY-MM-DD` while preserving the raw value as `<key>_raw`.
    - `document_type` to a controlled vocabulary while preserving raw value as `document_type_raw`.

    Args:
        fields: Extracted dokumentkopf fields.

    Returns:
        A new dict with best-effort normalized values.
    """
    normalized_fields: dict[str, Any] = _normalize_dokumentkopf_whitespace(fields)

    is_final_value = normalized_fields.get("is_final")
    if isinstance(is_final_value, str):
        normalized_value = is_final_value.strip().lower()
        if normalized_value in {"ja", "yes", "true"}:
            normalized_fields["is_final"] = True
        elif normalized_value in {"nein", "no", "false"}:
            normalized_fields["is_final"] = False

    normalized_fields = _normalize_dokumentkopf_date_fields(normalized_fields)
    normalized_fields = _normalize_dokumentkopf_document_type(normalized_fields)

    return normalized_fields


def _normalize_dokumentkopf_date_fields(fields: dict[str, Any]) -> dict[str, Any]:
    """Normalize date-like Dokumentkopf fields to ISO `YYYY-MM-DD`.

    This function is designed to avoid "we missed a case" rework later by being:
    - schema-explicit (only normalizes well-known date keys)
    - format-tolerant for common German date formats
    - lossless (preserves raw string under `<key>_raw`)
    - conservative (no conversion if parsing is ambiguous)

    Supported input formats (best-effort):
    - `DD.MM.YYYY`
    - `D.M.YYYY`
    - `DD.MM.YY` (interpreted as 19YY for >= 50, else 20YY)

    Args:
        fields: Dokumentkopf fields (possibly already partially normalized).

    Returns:
        New dict with normalized date strings and preserved raw values.
    """
    date_keys = {
        "decision_date",
        "execution_date",
        "valid_from",
        "valid_to",
        "version_date",
    }

    normalized_fields: dict[str, Any] = dict(fields)

    for date_key in sorted(date_keys):
        raw_value = normalized_fields.get(date_key)
        if not isinstance(raw_value, str):
            continue

        stripped_value = raw_value.strip()
        parsed_date = _parse_german_date_best_effort(stripped_value)
        if parsed_date is None:
            continue

        # Preserve raw value losslessly.
        raw_key = f"{date_key}_raw"
        if raw_key not in normalized_fields:
            normalized_fields[raw_key] = stripped_value

        normalized_fields[date_key] = parsed_date.isoformat()

    return normalized_fields


def _normalize_dokumentkopf_document_type(fields: dict[str, Any]) -> dict[str, Any]:
    """Normalize `document_type` to a small controlled vocabulary.

    This is intentionally conservative and lossless:
    - Only normalizes when the value is clearly one of the known types.
    - Preserves the original string in `document_type_raw` when normalization occurs.

    Controlled vocabulary (current, evidence-based):
    - `Beschluss`
    - `Verordnung`

    Args:
        fields: Dokumentkopf fields (possibly already partially normalized).

    Returns:
        New dict with normalized `document_type` and optional `document_type_raw`.
    """
    normalized_fields: dict[str, Any] = dict(fields)

    document_type_value = normalized_fields.get("document_type")
    if not isinstance(document_type_value, str):
        return normalized_fields

    stripped_value = document_type_value.replace("\u00a0", " ").strip()
    if not stripped_value:
        return normalized_fields

    canonical_map = {
        "beschluss": "Beschluss",
        "verordnung": "Verordnung",
    }
    canonical = canonical_map.get(stripped_value.lower())
    if canonical is None:
        return normalized_fields

    if (
        canonical != document_type_value
        and "document_type_raw" not in normalized_fields
    ):
        normalized_fields["document_type_raw"] = document_type_value

    normalized_fields["document_type"] = canonical
    return normalized_fields


_WHITESPACE_PATTERN = re.compile(r"\s+")


def _normalize_dokumentkopf_whitespace(fields: dict[str, Any]) -> dict[str, Any]:
    """Normalize whitespace in Dokumentkopf string values.

    This is a minimal, safe normalization layer:
    - converts NBSP to regular spaces
    - trims leading/trailing whitespace
    - collapses internal whitespace (spaces/tabs/newlines) to a single space

    Non-string values are returned unchanged.

    Args:
        fields: Dokumentkopf fields as extracted.

    Returns:
        A new dict with normalized string values.
    """
    normalized_fields: dict[str, Any] = {}
    for key, value in fields.items():
        if isinstance(value, str):
            normalized_text = value.replace("\u00a0", " ").strip()
            normalized_text = _WHITESPACE_PATTERN.sub(" ", normalized_text)
            normalized_fields[key] = normalized_text
        else:
            normalized_fields[key] = value
    return normalized_fields


_GERMAN_DATE_PATTERN = re.compile(r"^\s*(\d{1,2})\.(\d{1,2})\.(\d{2}|\d{4})\s*$")


def _parse_german_date_best_effort(value: str) -> date | None:
    """Parse common German date strings into a `date`.

    Args:
        value: Date string, typically like `10.11.1993`.

    Returns:
        `date` if parsing succeeded and is unambiguous, otherwise `None`.
    """
    match = _GERMAN_DATE_PATTERN.match(value)
    if match is None:
        return None

    day_value = int(match.group(1))
    month_value = int(match.group(2))
    year_token = match.group(3)

    if len(year_token) == 2:
        year_two_digits = int(year_token)
        # Best-effort heuristic: common for historical decisions.
        year_value = (
            1900 + year_two_digits if year_two_digits >= 50 else 2000 + year_two_digits
        )
    else:
        year_value = int(year_token)

    try:
        return date(year_value, month_value, day_value)
    except ValueError:
        return None


def _page_extract_best_effort(
    page: Page, *, max_text_chars: int, max_html_chars: int
) -> dict[str, str]:
    selectors = _best_effort_main_content_selector_candidates()

    selected_selector: str | None = None
    extracted_text: str | None = None
    extracted_html: str | None = None

    for selector in selectors:
        locator = page.locator(selector)
        try:
            count = locator.count()
        except Exception:
            continue
        if count <= 0:
            continue

        try:
            candidate_text = locator.first.inner_text(timeout=2_000)
            candidate_html = locator.first.inner_html(timeout=2_000)
        except Exception:
            continue

        # Heuristic: require some substance.
        if candidate_text and len(candidate_text.strip()) >= 200:
            selected_selector = selector
            extracted_text = candidate_text
            extracted_html = candidate_html
            break

    if extracted_text is None:
        extracted_text = page.inner_text("body")
    if extracted_html is None:
        extracted_html = page.content()

    return {
        "selected_selector": selected_selector or "",
        "extracted_text": _clip_text(extracted_text, max_text_chars),
        "extracted_html": _clip_text(extracted_html, max_html_chars),
    }


def _open_browser(*, headful: bool) -> tuple[Browser, Any]:
    playwright = sync_playwright().start()
    browser = playwright.chromium.launch(headless=not headful)
    return browser, playwright


def _close_browser(browser: Browser, playwright: Any) -> None:
    try:
        browser.close()
    finally:
        playwright.stop()


def _safe_timestamp_for_filename(timestamp_rfc3339: str) -> str:
    return (
        timestamp_rfc3339.replace(":", "")
        .replace("-", "")
        .replace("+", "Z")
        .replace(".", "_")
    )


def _write_json(path: Path, payload: dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def _append_jsonl(path: Path, payload: dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as handle:
        handle.write(json.dumps(payload, ensure_ascii=False) + "\n")


def main() -> int:
    """Run a bounded Playwright batch extraction over a Berlin sitemap snapshot.

    Returns:
        Exit code 0 on success, 1 if any documents failed, 2 for batch setup errors.
    """
    parser = argparse.ArgumentParser(
        prog="extract_from_sitemap_playwright.py",
        description="Batch extract Berlin portal documents from a sitemap snapshot via Playwright (Chromium).",
    )
    parser.add_argument(
        "--snapshot",
        default="",
        help="Path to a discovery snapshot JSON file. If omitted, uses latest under data/raw/de-state/berlin/discovery/.",
    )
    parser.add_argument(
        "--output-dir",
        default=str(DEFAULT_OUTPUT_DIRECTORY),
        help=f"Output directory (default: {DEFAULT_OUTPUT_DIRECTORY}).",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=DEFAULT_LIMIT,
        help=f"Number of documents to process (default: {DEFAULT_LIMIT}).",
    )
    parser.add_argument(
        "--timeout-seconds",
        type=float,
        default=DEFAULT_TIMEOUT_SECONDS,
        help=f"Per-run timeout in seconds (default: {DEFAULT_TIMEOUT_SECONDS}).",
    )
    parser.add_argument(
        "--wait-until",
        default=DEFAULT_WAIT_UNTIL,
        choices=["load", "domcontentloaded", "networkidle"],
        help=f"Navigation wait condition (default: {DEFAULT_WAIT_UNTIL}).",
    )
    parser.add_argument(
        "--extra-wait-ms",
        type=int,
        default=DEFAULT_EXTRA_WAIT_MS,
        help=f"Extra wait after navigation completes (default: {DEFAULT_EXTRA_WAIT_MS}).",
    )
    parser.add_argument(
        "--sleep-seconds",
        type=float,
        default=DEFAULT_SLEEP_SECONDS,
        help=f"Sleep between documents to be polite (default: {DEFAULT_SLEEP_SECONDS}).",
    )
    parser.add_argument(
        "--max-text-chars",
        type=int,
        default=DEFAULT_MAX_TEXT_CHARS,
        help=f"Max characters of extracted text per document (default: {DEFAULT_MAX_TEXT_CHARS}).",
    )
    parser.add_argument(
        "--max-html-chars",
        type=int,
        default=DEFAULT_MAX_HTML_CHARS,
        help=f"Max characters of extracted HTML per document (default: {DEFAULT_MAX_HTML_CHARS}).",
    )
    parser.add_argument(
        "--headful",
        type=str,
        default="false",
        help="Set to 'true' to run with a visible browser window (default: false).",
    )
    parser.add_argument(
        "--write-per-document",
        type=str,
        default="true",
        help="If 'true', write per-document JSON files in addition to JSONL (default: true).",
    )
    parser.add_argument(
        "--progress",
        type=str,
        default="true",
        help="If 'true', print per-document progress logs to stderr (default: true).",
    )
    parser.add_argument(
        "--progress-every",
        type=int,
        default=1,
        help="Print a summary line every N documents (default: 1).",
    )

    args = parser.parse_args()

    if args.limit <= 0:
        raise BatchExtractionError("--limit must be > 0")

    output_directory = Path(args.output_dir)
    _ensure_output_directory(output_directory)

    if args.snapshot:
        snapshot_path = Path(args.snapshot)
    else:
        snapshot_path = _find_latest_snapshot(DEFAULT_DISCOVERY_DIRECTORY)

    discovered_documents = _load_snapshot(snapshot_path)
    to_process = discovered_documents[: int(args.limit)]

    fetched_at = _utc_now_rfc3339()
    batch_id = _safe_timestamp_for_filename(fetched_at)

    jsonl_path = output_directory / f"berlin_playwright_batch_{batch_id}.jsonl"
    manifest_path = (
        output_directory / f"berlin_playwright_batch_{batch_id}_manifest.json"
    )

    headful_string = str(args.headful).strip().lower()
    headful = headful_string in {"1", "true", "yes", "y"}

    write_per_document_string = str(args.write_per_document).strip().lower()
    write_per_document = write_per_document_string in {"1", "true", "yes", "y"}

    progress_string = str(args.progress).strip().lower()
    progress_enabled = progress_string in {"1", "true", "yes", "y"}

    progress_every = int(args.progress_every)
    if progress_every <= 0:
        raise BatchExtractionError("--progress-every must be > 0")

    start_time = time.monotonic()

    browser: Browser | None = None
    playwright: Any | None = None

    successes = 0
    failures = 0

    per_doc_paths: list[str] = []
    processed_ids: list[str] = []

    try:
        browser, playwright = _open_browser(headful=headful)

        context = browser.new_context(
            user_agent=DEFAULT_USER_AGENT,
            ignore_https_errors=False,
            locale="de-DE",
        )

        navigation_timeout_ms = int(float(args.timeout_seconds) * 1000)

        if progress_enabled:
            prefix_counts: dict[str, int] = {}
            for discovered_document in to_process:
                document_id = discovered_document.document_id
                if document_id.startswith("jlr-"):
                    prefix = "jlr-"
                elif document_id.startswith("NJRE"):
                    prefix = "NJRE"
                else:
                    prefix = "other"
                prefix_counts[prefix] = prefix_counts.get(prefix, 0) + 1

            print(
                f"[berlin-playwright-batch] starting batch_id={batch_id} "
                f"docs={len(to_process)} snapshot={snapshot_path} "
                f"prefix_counts={prefix_counts}",
                file=sys.stderr,
            )

        for index, document in enumerate(to_process):
            processed_ids.append(document.document_id)

            page = context.new_page()
            page.set_default_timeout(navigation_timeout_ms)
            page.set_default_navigation_timeout(navigation_timeout_ms)

            doc_started = time.monotonic()

            record: dict[str, Any] = {
                "batch_id": batch_id,
                "fetched_at_rfc3339": fetched_at,
                "snapshot_path": str(snapshot_path),
                "index": index,
                "document_id": document.document_id,
                "canonical_url": document.canonical_url,
                "metadata": {},
                "status": "unknown",
                "timings": {},
                "extraction": {},
                "error": "",
            }

            try:
                url = document.canonical_url.strip()
                if not url.startswith("https://"):
                    raise BatchExtractionError(
                        f"Only https:// URLs are supported; got: {url}"
                    )

                if progress_enabled:
                    print(
                        f"[berlin-playwright-batch] {index + 1}/{len(to_process)} "
                        f"start document_id={document.document_id} url={url}",
                        file=sys.stderr,
                    )

                page.goto(url, wait_until=str(args.wait_until))
                page.wait_for_timeout(int(args.extra_wait_ms))

                metadata = _extract_minimal_metadata(
                    page, canonical_url=document.canonical_url
                )

                extraction = _page_extract_best_effort(
                    page,
                    max_text_chars=int(args.max_text_chars),
                    max_html_chars=int(args.max_html_chars),
                )

                record["status"] = "ok"
                record["metadata"] = metadata
                record["extraction"] = {
                    "selected_selector": extraction["selected_selector"],
                    "text_chars": len(extraction["extracted_text"]),
                    "html_chars": len(extraction["extracted_html"]),
                    "extracted_text": extraction["extracted_text"],
                    "extracted_html": extraction["extracted_html"],
                }

                successes += 1

                if progress_enabled:
                    print(
                        f"[berlin-playwright-batch] {index + 1}/{len(to_process)} "
                        f"ok document_id={document.document_id} "
                        f"text_chars={record['extraction']['text_chars']} "
                        f"html_chars={record['extraction']['html_chars']} "
                        f"selector={record['extraction']['selected_selector']!r}",
                        file=sys.stderr,
                    )

            except Exception as exception:
                failures += 1
                record["status"] = "error"
                record["error"] = f"{type(exception).__name__}: {exception}"

                if progress_enabled:
                    print(
                        f"[berlin-playwright-batch] {index + 1}/{len(to_process)} "
                        f"error document_id={document.document_id} "
                        f"error={record['error']}",
                        file=sys.stderr,
                    )

            finally:
                record["timings"] = {
                    "elapsed_seconds": round(time.monotonic() - doc_started, 3),
                    "wait_until": str(args.wait_until),
                    "extra_wait_ms": int(args.extra_wait_ms),
                }

                _append_jsonl(jsonl_path, record)

                if write_per_document:
                    doc_path = (
                        output_directory / f"{batch_id}_{document.document_id}.json"
                    )
                    _write_json(doc_path, record)
                    per_doc_paths.append(str(doc_path))

                with contextlib.suppress(Exception):
                    # Best effort; avoid aborting batch.
                    page.close()

                # Polite pacing between documents.
                if index < len(to_process) - 1:
                    time.sleep(max(0.0, float(args.sleep_seconds)))

                if progress_enabled and ((index + 1) % progress_every == 0):
                    elapsed_so_far_seconds = time.monotonic() - start_time
                    print(
                        f"[berlin-playwright-batch] progress "
                        f"{index + 1}/{len(to_process)} "
                        f"successes={successes} failures={failures} "
                        f"elapsed_seconds={round(elapsed_so_far_seconds, 3)}",
                        file=sys.stderr,
                    )

        elapsed_seconds = time.monotonic() - start_time

        manifest = {
            "batch_id": batch_id,
            "fetched_at_rfc3339": fetched_at,
            "snapshot_path": str(snapshot_path),
            "limit": int(args.limit),
            "processed_count": len(to_process),
            "successes": successes,
            "failures": failures,
            "elapsed_seconds": round(elapsed_seconds, 3),
            "jsonl_path": str(jsonl_path),
            "per_document_paths": per_doc_paths,
            "processed_document_ids": processed_ids,
        }
        _write_json(manifest_path, manifest)

        if progress_enabled:
            print(
                f"[berlin-playwright-batch] finished batch_id={batch_id} "
                f"processed={len(to_process)} successes={successes} failures={failures} "
                f"elapsed_seconds={round(elapsed_seconds, 3)} manifest={manifest_path}",
                file=sys.stderr,
            )

        # Print only the manifest path (stable artifact entrypoint).
        print(str(manifest_path))
        return 0 if failures == 0 else 1

    finally:
        if browser is not None and playwright is not None:
            _close_browser(browser, playwright)


if __name__ == "__main__":
    try:
        raise SystemExit(main())
    except BatchExtractionError as exception:
        print(f"BatchExtractionError: {exception}", file=sys.stderr)
        raise SystemExit(2) from exception
